Deep-learning models like Convolutional Neural Networks (CNN) and Graph Convolutional Network(GCN) are often used in 3D point cloud processing. Some existing work tend to explore the robustness of 3D PCSS models by generating and using adversarial attack. While in the real world autonomous driving scene, a set of sensors may be using to help detect the driving environment. As for security analysis, we assume the attacker has white-box access to the driving related sensor  messages, include 2D images and 3D point clouds usually used in objective recognition and semantic segmentation which is very useful for self-driving decision. In this paper we consider different domain learning complement information to help improve the performance under attack.

## **Problem Definition**

In this section , we give a formal definition of point clouds and the attacker's goal.  

We define input source samples ${X_S^{2D},X_S^{3D},Y_S^{3D}\in\mathscr{S}}$ and target samples ${X_T^{2D},X_T^{3D}\in\mathscr{T}}$.  where $X^{2D}$ represents the image and $X^{3D}$ represents the corresponding point cloud,  with 3D points in the camera reference frame.  A point cloud can be defined as a set of N points,  i.e. $\{p_i\}_{i=1}^N$.  Where each points $p_i=(pos_x,pos_y,pos_z)$ present the location information of received point . 

Note that $X^{3D}$ contains only points visible from the RGB camera,  assuming that the calibration of the LiDAR and Camera is available for both domains and does not change over time.

Given the samples of source domain $\mathscr{S}$and target domain $\mathscr{T}$, we intend to learn a function $f:\mathscr{S}\cup\mathscr{T}\rightarrow{Y_T^{3D}}$ for 3D semantic segmentation in the target domain.



Below we formalize the attack goals as generalized used in adversarial attack. (need citation...). The segmentation model is used to mapping the input point cloud $X=\{x_i|i=1...N,x_i\in\mathscr{X}\}$ to the labels of all points $Y=\{y_i|i=1...N,y_i\in \mathscr{Y}\}$. $\mathscr{X}$ is the real-time points  sampled by the LiDAR system and sent to the semantic segmentation system, while $\mathscr{Y}$ is a set of feasible labels using in classification. We have established methodologies based on norm-bounded principles for both targeted and untargeted attacks, highlighting potential limitations in the attack scenarios. It is worth noting that encountering limitations in attacks is a commonplace occurrence in real-life situations.



## **ATTACK SCENARIO AND THREAT MODEL**

#### 1. Attack scenario

In the real world, the victim autonomous car is usually moving when the attacker tries to perform the attack. As shown in the Figure{}, the autonomous proxy tend to collect a series consecutive frames of point cloud data and generate perception result for each frame. In the semantic segmentation case,  the class-wise labels generated by the system are used to further decision proccess. To ensure a successful attack goal, the attacker needs to find a way to change the perception result for a conductive frames and lead to a wrong decision made by the system. While there still exists a domain shift problem  in the target domain when using these adversarial samples.

Some relate papers (\cite{}) point out the attack samples are transferable in 2D image when conducting target attack and untargetted attack. And in 3D point cloud deal model, (\cite{}) design an Imperceptible transfer attack on 3D Point Cloud Classification and thus inspire us to investigate the possibillty of 3D semantic segmentation adversarial sampels in 3D PCSS models.  By utilizing the source 3D data, source 3D class-wise label and the target model, attackers can pre-train the adversarial samples in advance while using in the specific attack scene and time.



#### 2. Threat Model

**Adversary's Goals.** We consider the  LiDAR spoofing attack , assume the attacker has white-box access to the machine learning model and the perception system. The attacker aim to change the semantic segmentation result from the PCSS models loaded in autonomous proxies like vehicles and delivery robots happened in indoor or outdoor scenario.

 The Attacker tend to pre-train and use adversarial samples to conduct the attack, while the capability of attacking is limited in some cases when attackers expect the attack is imperceptible. We consider this threat model reasonable since the attacker could access the perception system and source data by additional engineering efforts like reverse engineering the software through Internet or hardware hacking.

In this paper, we explore two types of attacks: Targeted Attacks and Untargeted Attacks, which proves to be effective in former works.

**Targeted Attack**. In this setting, the attacker have an access to the target data and target domain semantic segmentation result, which attacking object information (location,...) can be exposed to the attacker. Thus the attacker can  choose a specific subset of attack points $X_{T}=\{x_{i}|i\in{T},x_{i}\in \mathscr{X}\}$. $T$ is the indices of the the item which the attacker try to change the predicted labels to $Y_T=\{y_i|i\in T,y_i\in{\mathscr{Y}}\}$. For a point $x_i=\{p_i\}$, we assume the attacker conduct the model attack by perturb the coordinates of the point with neccessary constraints, In which Point cloud coordinate perturbation have been generalized investigated. The perturbation values on the original points can be represent as $R={r_i|i\in T}$ , and the new point cloud will be 



## Scene

”无地图区域，雷达信号弱的区域“ 语义分割主导地位



## Discussion

**为什么只攻击点云数据，不攻击2D image数据?**

针对2D图像的攻击和防御策略已经被广泛研究，针对3D点云攻击的防御策略还未被提出。



**源域数据是否存在?**

源域数据存在，常见被攻击车辆的行驶方式采集数据

1、模拟被攻击车辆的驾驶行为（正常行驶场景），采集大量数据用于分割。

2、在采集的数据上随机位置加上对抗点云，两万个点云图像在随机加对抗点，输入攻击框架进行训练。根据设定损失函数最小化找出对抗点云的最佳位置（基于我们给出的攻击目标）。

**实时场景下的攻击有效, 车辆位置变换攻击是否有效？**

Attacking moving vehicle

![image-1](images/image-1.png)

![image-2](images/image-2.png)

**攻击的是任务域数据还是源域数据？**

攻击对象：从源域进行攻击，生成对抗样本

"Adversarial Attacks against LiDAR Semantic Segmentation in Autonomous Driving" ‘22

对抗样本：在原始点云图像，模型参数训练

Question:仅从任务域数据生成？

不行，因为对抗性点簇的位置是提前离线生成的，需要很长时间，而目标域是实时捕获的点云数据，我们无法在很短的时间内生成位置

|从源域生成的对抗性点簇在任务域可能存在领域迁移的问题|，攻击框架迁移

**在原有的框架加2Dheader**

potential defense strategy by cross-modal structure

SemanticKITTI

3D数据集->2D数据集+3D数据集

工作：

Uni-modal->Multi-modal

1、“Adversarial Attacks against LiDAR Semantic Segmentation in Autonomous Driving” 中提出的攻击方式是“the first study on physically realizable adversarial attacks against LiDAR point cloud semantic segmentation with real-world evaluations.”，目前还没有其他可落地的对抗攻击方式，所以，我们针对这种特定的攻击来进行防御。

1、 存在对抗样本生成攻击方式，在模型和源数据

2、过敏？攻击未发生情况 

3、防御策略相关：进行对抗训练使用数据增强（random resizing and padding with a given probability）

语义分割模型$\rightarrow$分类模型(point-wise)

攻击策略：

基于白盒攻击模型（模型参数和源域数据已知的情况）。在源域3D点云数据，点云类别标签，源域模型已知的情况下（通常黑客也可以通过额外的工作类似于逆向工程来获取系统信息）进行对抗训练得到对抗样本；通过在任务域（无标签）叠加对抗样本来实现攻击(attack objective)。

防御策略：

1、利用类似于X-MUDA模型（cross-domain,cross-modal）学习不同模态信息来进行防御，（多传感器融合作为一种防御策略的可行性）



